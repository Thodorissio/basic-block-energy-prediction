{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Block Energy Prediction Consumption Demo Notebook\n",
    "\n",
    "This notebook contains the core guidelines and structure that were followed for the implementation of the basic block energy consumption system. The source code used, resides inside the bb_energy_prediction directory.\n",
    "\n",
    "The distinct machine learning models that were developed are the following:\n",
    "* LSTM with PalmTree embeddings as input\n",
    "* Simple Dense network with PalmTree embeddings as input\n",
    "* LSTM with custom vocabulary and embedding layer \n",
    "* Linear Regression\n",
    "* Lasso Regression\n",
    "* Ridge Regression\n",
    "* ElasticNet\n",
    "* SGD\n",
    "* SVR\n",
    "* Hist-Gradient Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, ElasticNet, BayesianRidge, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from bb_energy_prediction import models, data_utils, train, evaluate, sklearn_regressors\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset. If the dataset does not exist then it is created automatically and saved inside the data path. The dataset columns include:\n",
    "* bb: The basic blocks\n",
    "* energy: The energy label\n",
    "* program_name: The benchmark program that basic blocks originate\n",
    "* bb_embeddings: The pre-computed PalmTree embeddings\n",
    "\n",
    "Warning: If creating the data file, the PalmTree embeddings ingestion require several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_utils.get_data_df(data_path=\"../energy_data/data.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the custom embedding approach a vocabulary should be created and the dataset should be tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = data_utils.get_inst_vocab(data_df)\n",
    "data_df[\"encoded_bb\"] = data_df.bb.apply(lambda x: data_utils.encode_bb_from_vocab(x, vocab, max_insts=20))\n",
    "print(f\"size of vocabulary: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into test and train_val sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "test_size = int(0.1 * len(shuffled_data_df))\n",
    "test_df = shuffled_data_df[-test_size:]\n",
    "train_val_df = shuffled_data_df[:-test_size]\n",
    "\n",
    "print(f\"Test data size: {len(test_df)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model_checkpoints file contains the best attributes for the implemented models, after hyperparameter optimization using Optuna."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the deep learning approaches follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save and load flags\n",
    "load = True\n",
    "save = False\n",
    "\n",
    "#Specify the desired model path\n",
    "exp_dir = \"../model_checkpoints/lstm_vocab_models/base_model\"\n",
    "with open(f\"{exp_dir}/additional_attributes.json\") as json_file:\n",
    "    model_config = json.load(json_file)\n",
    "\n",
    "model_params = model_config[\"model_params\"]\n",
    "train_params = model_config[\"train_params\"]\n",
    "batch_size = model_config[\"batch_size\"]\n",
    "\n",
    "#enc_type can be \"vocab\" or \"palmtree\" depending on model choice\n",
    "data_loaders = data_utils.get_data_dict(\n",
    "    data_df=train_val_df, batch_size=batch_size, enc_type=\"vocab\"\n",
    ")\n",
    "train_loader = data_loaders[\"train_loader\"]\n",
    "val_loader = data_loaders[\"val_loader\"]\n",
    "\n",
    "model = models.LSTM_Regressor(vocab_size=len(vocab), custom_embs=True, **model_params)\n",
    "\n",
    "if load:\n",
    "    model.load_state_dict(torch.load(f\"{exp_dir}/model\"))\n",
    "    model.cuda()\n",
    "    train_results = {}\n",
    "    train_results[\"train_loss\"] = model_config[\"train_loss\"]\n",
    "    train_results[\"val_loss\"] = model_config[\"val_loss\"]\n",
    "else:\n",
    "    train_results = train.train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        verbose=True,\n",
    "        **train_params,\n",
    "    )\n",
    "    if save:\n",
    "        torch.save(model.state_dict(), f\"{exp_dir}/model\")\n",
    "        additional_attributes = {\n",
    "            \"model_params\": model_params,\n",
    "            \"train_params\": train_params,\n",
    "            \"loss\": \"RMSE\",\n",
    "            \"batch_size\": batch_size,\n",
    "            \"number of data\": len(train_loader)*batch_size,\n",
    "            \"train_loss\": train_results[\"train_loss\"],\n",
    "            \"val_loss\": train_results[\"val_loss\"],\n",
    "        }\n",
    "        with open(f\"{exp_dir}/additional_attributes.json\", \"w\") as file:\n",
    "            json.dump(additional_attributes, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_results[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(train_results[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Train and Val losses\")\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sklearn approaches choose among the available regressors and follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([\" \".join(bb) for bb in train_val_df.bb.tolist()])\n",
    "y_train = train_val_df.energy.values\n",
    "\n",
    "X_test = np.array([\" \".join(bb) for bb in test_df.bb.tolist()])\n",
    "y_test = test_df.energy.values\n",
    "\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit_transform(X_train)\n",
    "vocab_len = len(cnt_vect.get_feature_names_out())\n",
    "print(f\"Vocab length: {vocab_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "load = True\n",
    "\n",
    "#Specify the desired sklearn regressor path\n",
    "exp_dir = \"../model_checkpoints/regressors/svr\"\n",
    "\n",
    "with open(f\"{exp_dir}/additional_attributes.json\") as json_file:\n",
    "    model_config = json.load(json_file)\n",
    "\n",
    "pipe_params = model_config[\"pipe_params\"]\n",
    "\n",
    "if load:\n",
    "    pipe = joblib.load(f\"{exp_dir}/pipe\")\n",
    "else:\n",
    "    #change for the desired sklearn regressor\n",
    "    regressor = SVR()\n",
    "\n",
    "    pipe = sklearn_regressors.train_pipe(regressor, X_train, y_train, **pipe_params)\n",
    "\n",
    "    if save:\n",
    "        additional_attributes = {\n",
    "            \"pipe_params\": pipe_params,\n",
    "        }\n",
    "\n",
    "        joblib.dump(pipe, f\"{exp_dir}/pipe\")\n",
    "        with open(f\"{exp_dir}/additional_attributes.json\", \"w\") as file:\n",
    "            json.dump(additional_attributes, file, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the energy predictions of the test set and create the evaluation visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use flag to choose between sklearn and deep learning models\n",
    "deep_learning = True\n",
    "\n",
    "if deep_learning:\n",
    "    if model.custom:\n",
    "        test_preds = evaluate.predict(model=model, test_bbs=test_df.bb.tolist(), vocab=vocab)\n",
    "    else:\n",
    "        test_embs = [emb.tolist() for emb in test_df.bb_embeddings.tolist()]\n",
    "        test_preds = evaluate.predict(model=model, test_bbs=test_embs)\n",
    "else:\n",
    "    test_bbs = np.array([\" \".join(bb) for bb in test_df.bb.tolist()])\n",
    "    test_preds = pipe.predict(test_bbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_energies = test_df.energy.values\n",
    "mae = round(mean_absolute_error(true_energies, test_preds), 3)\n",
    "\n",
    "print(f\"Mean absolute error: {mae} (*61μJ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "plt.suptitle(f\"Basic blocks' energy histogram label data vs model preds for test set\")\n",
    "axs[0].hist(true_energies, range=(0, 10), bins=50)\n",
    "axs[0].set_title('labels')\n",
    "axs[0].set_xlabel('Energy (*61μJ)')\n",
    "axs[1].hist(test_preds, range=(0, 10), bins=50)\n",
    "axs[1].set_title('Model preds')\n",
    "axs[1].set_xlabel('Energy (*61μJ)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy-prediction",
   "language": "python",
   "name": "energy-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
